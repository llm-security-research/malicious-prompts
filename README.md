# Malignant and PromptSentinel - Solution for Jailbreak Attacks Against Large Language Models

## Authors

[Vinicius Krieger Granemann](https://github.com/Hermitao)

[Osmary Camila Bortoncello Glober](https://github.com/marycamila184)

# Downloads

## Downloading the Malignant dataset

All versions of the Malignant dataset are located in the Malignant directory.

## Downloading the PromptSentinel models

All trained models cited in the paper can be used through PyTorch Hub. The PyTorch model files are also located in PromptSentinel/.

# Training

If you wish to train your own models in a similar fashion or replicate this research, you can follow thes steps:

# Citation

If you find our work useful, please [cite our paper](https://github.com/llm-security-research/malicious-prompts): 

```
@misc{krieger2024malignant,
      title={Malignant and PromptSentinel - Solution for Jailbreak Attacks Against Large Language Models}, 
      author={Vinicius Krieger Graneman and Osmary Camila Bortoncello Glober},
      year={2024},
      eprint={},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
```