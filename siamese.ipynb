{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import importlib\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size:\t\t\t 384\n",
      "Number of embeddings:\t\t 1496\n",
      "Number of category entries:\t 1496\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "INPUT_FILE = \"data/processed/maleficent.csv\"\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "df.head()\n",
    "\n",
    "embeddings = []\n",
    "for embedding in df['embedding']:\n",
    "    temp = [float(x.strip(' []')) for x in embedding.split(',')]\n",
    "    embeddings.append(temp)\n",
    "\n",
    "categories = []\n",
    "for category in df['category']:\n",
    "    categories.append(category)\n",
    "\n",
    "print(\"Vector size:\\t\\t\\t\", len(embeddings[0]))\n",
    "print(\"Number of embeddings:\\t\\t\", len(embeddings))\n",
    "print(\"Number of category entries:\\t\", len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SiameseNetwork(\n",
      "  (embedding): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=512, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from siamese_network import SiameseNetwork\n",
    "\n",
    "print(SiameseNetwork(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    category = categories[i]\n",
    "\n",
    "    if category not in data_dict:\n",
    "        data_dict[category] = []\n",
    "\n",
    "    # data_dict[category].append(list(map(float, embedding)))\n",
    "    data_dict[category].append(embedding)\n",
    "\n",
    "\n",
    "data_dict = {k: v for k, v in data_dict.items() if len(v) > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1402\n",
      "70\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "print(len(data_dict['conversation']))\n",
    "print(len(data_dict['jailbreak']))\n",
    "print(len(data_dict['act_as']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from triplet_dataset import TripletDataset\n",
    "\n",
    "\n",
    "siamese_dataset = TripletDataset(data_dict, False)\n",
    "\n",
    "# Load the training dataset\n",
    "num_workers = 4\n",
    "batch_size = 1800\n",
    "train_dataloader = DataLoader(\n",
    "    siamese_dataset, shuffle=True, num_workers=num_workers, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SiameseNetwork(\n",
      "  (embedding): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=512, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of epochs: 50\n",
      "Embedding dimension: 128\n",
      "Optimizer: Adam\n",
      "Learning rate: 0.0001\n",
      "Loss function: TripletMarginLoss\n",
      "Margin: 0.3\n",
      "Batch size: 1800\n",
      "Number of workers: 4\n",
      "Dataset: data/processed/maleficent.csv\n",
      "Number of classes: 3\n",
      "Number of samples: 1496\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "embedding_dim = 128\n",
    "\n",
    "siamese_net = SiameseNetwork(embedding_dim).cuda()\n",
    "margin = 0.3\n",
    "criterion = nn.TripletMarginLoss(margin=margin)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "optimizer = optim.Adam(siamese_net.parameters(), lr=learning_rate)\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "print(siamese_net)\n",
    "print(f\"Number of epochs: {n_epochs}\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "print(f\"Optimizer: Adam\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Loss function: TripletMarginLoss\")\n",
    "print(f\"Margin: {margin}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Number of workers: {num_workers}\")\n",
    "print(f\"Dataset: {INPUT_FILE}\")\n",
    "print(f\"Number of classes: {3}\")\n",
    "print(f\"Number of samples: {len(siamese_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, model, optimizer, criterion):\n",
    "    global train_dataloader\n",
    "    best_loss = float(\"inf\")\n",
    "    best_state = None\n",
    "    iteration_number = 0\n",
    "    counter = []\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Iterate over batches\n",
    "        for i, (anchor, positive, negative) in enumerate(train_dataloader, 0):\n",
    "            # Send the images to CUDA\n",
    "            anchor, positive, negative = anchor.cuda(), positive.cuda(), negative.cuda()\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Pass in the anchor, positive, and negative images into the network\n",
    "            output_anchor, output_positive, output_negative = model(anchor, positive, negative)\n",
    "\n",
    "            # Compute the triplet loss\n",
    "            loss_triplet = criterion(output_anchor, output_positive, output_negative)\n",
    "\n",
    "            # If the validation loss is at a minimum\n",
    "            if loss_triplet < best_loss:\n",
    "                best_loss = loss_triplet\n",
    "                best_state = model.state_dict()\n",
    "\n",
    "            # Backpropagation\n",
    "            loss_triplet.backward()\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.step()\n",
    "\n",
    "            # Every 10 batches print out the loss\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch number {epoch}\\n Current loss {loss_triplet.item()}\\n\")\n",
    "                iteration_number += 10\n",
    "\n",
    "                counter.append(iteration_number)\n",
    "                loss_history.append(loss_triplet.item())\n",
    "\n",
    "    print(\"Training finished\")\n",
    "    # print(counter)\n",
    "    # print(loss_history)\n",
    "    return counter, loss_history, best_state, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(iteration, loss):\n",
    "    plt.plot(iteration, loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0\n",
      " Current loss 0.2805124819278717\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 0.26904165744781494\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 0.2616995871067047\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 0.2540184259414673\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 0.2438860386610031\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 0.22982053458690643\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 0.21692660450935364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = time.time()\n",
    "# Training\n",
    "counter, loss_history, best_state, best_loss = train(n_epochs, siamese_net, optimizer, criterion)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "# Transforme seconds into minutes\n",
    "\n",
    "minutes, seconds = divmod(execution_time, 60)\n",
    "\n",
    "show_plot(counter, loss_history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "\n",
    "# create folder\n",
    "model_dir = f\"trained/{now}\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "# Save the trained model\n",
    "torch.save(siamese_net.state_dict(), os.path.join(model_dir, \"state.pth\"))\n",
    "\n",
    "# Save the best state\n",
    "torch.save(best_state, os.path.join(model_dir, f\"best_state-{best_loss}.pth\"))\n",
    "\n",
    "torch.save(siamese_net, os.path.join(model_dir, \"model.pth\"))\n",
    "with open(os.path.join(model_dir, \"Architecture.txt\"), \"w\") as f:\n",
    "    f.write(str(siamese_net))\n",
    "    f.write(f\"\\nExecution time: {int(minutes)} minutes and {seconds:.2f} seconds\\n\")\n",
    "    f.write(f\"Number of epochs: {n_epochs}\\n\")\n",
    "    f.write(f\"Embedding dimension: {embedding_dim}\\n\")\n",
    "    f.write(f\"Optimizer: Adam\\n\")\n",
    "    f.write(f\"Learning rate: {learning_rate}\\n\")\n",
    "    f.write(f\"Loss function: TripletMarginLoss\\n\")\n",
    "    f.write(f\"Margin: {margin}\\n\")\n",
    "    f.write(f\"Batch size: {batch_size}\\n\")\n",
    "    f.write(f\"Number of workers: {num_workers}\\n\")\n",
    "    f.write(f\"Dataset: {INPUT_FILE}\\n\")\n",
    "    f.write(f\"Number of classes: {len(csv_data)}\\n\")\n",
    "    f.write(f\"Number of samples: {len(siamese_dataset)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
